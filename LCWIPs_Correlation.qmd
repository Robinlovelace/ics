---
title: Working with both the UK Transport Authorities Dataset and the LCWIPs dataset
jupyter: python3
format: html
---



```{python}
pip install pandas geopandas requests matplotlib folium scipy --quiet
```

```{python}
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib as plt
```

```{python}
import requests
import folium
import json
```

```{python}
import matplotlib.pyplot as plt  # Importing matplotlib's pyplot
import seaborn as sns
```

##loading the LCWIP Json file first

```{python}
with open('LCWIP_database_v1.json') as f: #reading the lcwip database
  lcwip_data = json.load(f)
  lcwip_df = pd.DataFrame(lcwip_data)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
df_org = lcwip_df #saving the untouched dataset
print(lcwip_df.columns.tolist()) #checking the columns
```

```{python}
lcwip_pct = df_org #df just for the mentions of PCT
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print(lcwip_pct['mentions_pct'].value_counts()) #checking the counts of reports that mention PCT
print(lcwip_pct['mentions_pct'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%') #into percentage
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
lcwip_pct.columns
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 489}
lcwip_pct['local_authority_name'].value_counts()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 489}
lcwip_pct['report_name'].value_counts()
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
lcwip_pct['local_authority_name'].tolist()
```

### Loading the UK Transport Authorities boundary

```{python}
atf_df = gpd.read_file('https://raw.githubusercontent.com/itsleeds/uktransportauthorities/main/atf_joined_2025.geojson') #loading the geojson atf
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 0}
atf_df.head()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 0}
atf_df.count() #checking the counts to be consistent with the csv
```

```{python}
atf_csv = pd.read_csv('https://raw.githubusercontent.com/itsleeds/uktransportauthorities/main/atf_table_aggregated.csv') #loading the csv file
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 0}
atf_csv.head()
```

```{python}
name1 = atf_csv['name']
name2 = atf_df['name']
if name1 is name2:
    print("The datasets are in same order.") #making sure if there are same set of names or count of entries in both datasets.
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
atf_csv.info()
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
atf_df.columns.tolist()
```

```{python}
cols_df = ['atf24_25', 'atf23_24', 'atf22_23', 'atf21_22', 'atf20_21', 'atfe20_21']
for col in cols_df:
    num_col1 = pd.to_numeric(atf_df[col], downcast='integer', errors='coerce')
    atf_df[col] = num_col1.astype('Int64')

    num_col2 = pd.to_numeric(atf_csv[col], downcast='integer', errors='coerce')
    atf_csv[col] = num_col2.astype('Int64')
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
atf_df.info()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 0}
atf_csv.head()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 0}
atf_df.head()
```

```{python}
atf_subset = atf_csv[["name","name_atf"]] #creating a subset from csv to merge into the geodataframe
```

```{python}
atf_df_new = atf_df.merge(atf_subset,how='left',on='name') #merging on the name column using left instead of inner key
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 0}
atf_df_new.head()
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
atf_df_new.info()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 0}
atf_df_new.plot()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 0}
atf_df.plot()
```

### Matching the LAs between the two final dfs

```{python}
#| colab: {base_uri: https://localhost:8080/}
atf_df_new.crs
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 81}
atf_df_new[atf_df_new['name_atf'].duplicated()]
```

```{python}
atf_names_list = atf_df_new['name'].dropna().tolist()
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
atf_names_list
```

```{python}
ca_constituent_map = {
    'West Yorkshire Combined Authority': ['Leeds', 'Wakefield', 'Calderdale', 'Kirklees', 'Bradford'],
    'Liverpool City Region Combined Authority': ['Halton', 'Knowsley', 'Liverpool', 'Sefton', 'St Helens', 'Wirral'],
    'Greater Manchester Combined Authority': ['Bolton', 'Bury', 'Manchester', 'Oldham', 'Rochdale', 'Salford', 'Stockport', 'Tameside', 'Trafford', 'Wigan'],
    'West of England Combined Authority': ['Bath & North East Somerset', 'Bristol', 'South Gloucestershire'],
    'North East Joint Transport Committee': ['Durham', 'Gateshead', 'South Tyneside', 'Sunderland', 'Northumberland', 'Newcastle upon Tyne', 'North Tyneside'],
    'South Yorkshire Combined Authority': ['Barnsley', 'Doncaster', 'Rotherham', 'Sheffield'],
    'Tees Valley Combined Authority': ['Darlington', 'Hartlepool', 'Middlesbrough', 'Redcar and Cleveland', 'Stockton-on-Tees'],
    'Cambridgeshire and Peterborough Combined Authority': ['Cambridge', 'Peterborough'],
    'West Midlands ITA': ['Birmingham', 'Coventry', 'Dudley', 'Sandwell', 'Solihull', 'Walsall', 'Wolverhampton'],
    'East Midlands Combined Authority': ['Derby', 'Derbyshire', 'Nottingham', 'Nottinghamshire','Leicester','Leicestershire','Lincolnshire','Nottingham','Nottinghamshire','Rutland'],
    'Royal Borough of Windsor and Maidenhead Unitary Authority':['The Royal Borough of Windsor & Maidenhead (RBWM)','Windsor and Maidenhead','Royal Borough of Windsor & Maidenhead'],
    'Brighton & Hove Unitary Authority':['Brighton and Hove','Brighton & Hove City Council'],
    'Isles of Scilly':['Council of the ISLES OF SCILLY'],
    'Hampshire':['Hart District Council','Hampshire County Council'],
    'Cumbria':["Cumbria County Council","South Lakeland District Council", "Kendal Town Council","Cumbria County Council, South Lakeland District Council, and Kendal Town Council [3]"]
}
```

```{python}
def find_authority_match(lcwip_name, atf_list, ca_map):
    """
    Matches an LCWIP authority name to an ATF authority name.

    1. Checks for a direct match with a Combined Authority name.
    2. If no direct match, checks if any constituent council name is present.
    3. If still no match, checks for a direct match with other local authorities.
    """
    if not isinstance(lcwip_name, str):
        return None

    # Priority 1: Check for a direct match with a Combined Authority from the map keys
    for ca_name in ca_map.keys():
        if ca_name in lcwip_name:
            return ca_name

    # Priority 2: Check for constituent councils from the map values
    for ca_name, councils in ca_map.items():
        for council in councils:
            if council in lcwip_name:
                return ca_name # Return the parent Combined Authority name

    # Priority 3: Check for a direct match with any other name in the ATF list
    # Sort by length to match "North Yorkshire" before "York"
    for atf_name in sorted(atf_list, key=len, reverse=True):
        if atf_name in lcwip_name:
            return atf_name

    return None
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
#| collapsed: true
#Apply the function to your DataFrame
lcwip_pct['matched_authority_name'] = lcwip_pct['local_authority_name'].apply(
    lambda x: find_authority_match(x, atf_names_list, ca_constituent_map)
)

#Display the results to verify
print("Matching complete. Here are the unique matches found:")
print(
    lcwip_pct[['local_authority_name', 'matched_authority_name']]
    .dropna()
    .drop_duplicates()
    .sort_values('matched_authority_name')
)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 423}
lcwip_pct[['local_authority_name', 'matched_authority_name']]
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 458}
lcwip_pct['matched_authority_name']
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 523}
lcwip_pct[lcwip_pct['matched_authority_name'].isna()]['local_authority_name']
```

```{python}
lcwip_pct.columns
```

```{python}
lcwip_pct.head()
```

```{python}
lcwip_pct.info()
```

```{python}
# Perform the aggregation
lcwip_summary = lcwip_pct.groupby('matched_authority_name').agg(
    lcwip_report_count=('report_name', 'size'),
    pct_mentions=('mentions_pct', 'sum'),
    total_funding_gbp=('total_cost_pounds', 'sum'),
    total_length_km=('length_of_network_km', 'sum')
).reset_index() # reset_index turns the grouped output back into a DataFrame

# Calculate the PCT usage percentage
lcwip_summary['pct_usage_percentage'] = (lcwip_summary['pct_mentions'] / lcwip_summary['lcwip_report_count'] * 100).round(1)

print("LCWIP data has been aggregated by authority:")
print(lcwip_summary.head())
```

```{python}
lcwip_summary.info()
```

```{python}
#merge on the 'name_atf' column from the geojson and the 'matched_authority' from our summary.
atf_spatial_analysis_df = atf_df_new.merge(
    lcwip_summary,
    left_on='name',
    right_on='matched_authority_name',
    how='left'
)

#Clean up the final DataFrame
# The merge will create a duplicate 'matched_authority' column, so we'll drop it.
atf_spatial_analysis_df.drop('matched_authority_name', axis=1, inplace=True)

atf_lcwip_corr = atf_spatial_analysis_df

# Replace NaN values in the numeric columns with 0 for cleaner mapping and analysis.
cols_to_fill = ['lcwip_report_count', 'pct_mentions', 'total_funding_gbp', 'total_length_km', 'pct_usage_percentage']
for col in cols_to_fill:
    atf_spatial_analysis_df[col] = atf_spatial_analysis_df[col].fillna(0)


print("\nMerge complete. Your GeoDataFrame now contains the aggregated LCWIP analysis.")
# Display the new columns for a few authorities to verify the merge
print(atf_spatial_analysis_df[['name_atf', 'lcwip_report_count', 'pct_usage_percentage', 'total_funding_gbp']].head())
```

```{python}
atf_spatial_analysis_df.info()
```

```{python}
atf_spatial_analysis_df['atf_total_20_24'] = atf_spatial_analysis_df['atf24_25'] + atf_spatial_analysis_df['atf23_24'] + atf_spatial_analysis_df['atf22_23'] + atf_spatial_analysis_df['atf21_22'] + atf_spatial_analysis_df['atf20_21']
```

```{python}
atf_spatial_analysis_df.head()
```

```{python}
import folium
```

```{python}
# Clean up data types for the popup
atf_spatial_analysis_df['lcwip_report_count'] = atf_spatial_analysis_df['lcwip_report_count'].astype(int)
atf_spatial_analysis_df['pct_mentions'] = atf_spatial_analysis_df['pct_mentions'].astype(int)
atf_spatial_analysis_df['atf24_25'] = pd.to_numeric(atf_spatial_analysis_df['atf24_25'], errors='coerce').fillna(0).astype(int)


#Creating the Interactive Map
m = folium.Map(location=[52.9, -1.5], zoom_start=6, tiles='CartoDB positron')

# Define a custom threshold scale for coloring
threshold_scale = [0, 1, 2, 4, 6, 8, 11]

folium.Choropleth(
    geo_data=atf_spatial_analysis_df,
    name='LCWIP Report Count',
    data=atf_spatial_analysis_df,
    columns=['name', 'lcwip_report_count'],
    key_on='feature.properties.name',
    fill_color='YlGnBu',
    fill_opacity=0.7,
    line_opacity=0.4,
    legend_name='Number of LCWIP Reports in Database',
    threshold_scale=threshold_scale,
    nan_fill_color="white"
).add_to(m)

#Add the Custom Popups
# Define the specific fields and aliases you want to display
popup_fields = ['name', 'lcwip_report_count', 'pct_mentions', 'total_funding_gbp', 'atf24_25']
aliases = [
    'Authority:',
    'LCWIP Report Count:',
    'Reports Mentioning PCT:',
    'Total LCWIP Funding:',
    'ATF 24/25 Allocation:'
]

# Add a GeoJson layer with the custom popups
popup = folium.features.GeoJsonPopup(fields=popup_fields, aliases=aliases, localize=True, labels=True)
folium.GeoJson(
    atf_spatial_analysis_df,
    name="Authority Details",
    style_function=lambda x: {'color': 'black', 'weight': 0.5, 'fillOpacity': 0.1},
    tooltip=folium.features.GeoJsonTooltip(fields=['name']),
    popup=popup
).add_to(m)

folium.LayerControl().add_to(m)

#Display the Map
m
```

```{python}
# Clean up data types for the popup
atf_spatial_analysis_df['lcwip_report_count'] = atf_spatial_analysis_df['lcwip_report_count'].astype(int)
atf_spatial_analysis_df['pct_mentions'] = atf_spatial_analysis_df['pct_mentions'].astype(int)
atf_spatial_analysis_df['atf24_25'] = pd.to_numeric(atf_spatial_analysis_df['atf24_25'], errors='coerce').fillna(0).astype(int)


#Creating the Interactive Map
m = folium.Map(location=[52.9, -1.5], zoom_start=6, tiles='CartoDB positron')

# Define a custom threshold scale for coloring
threshold_scale = [0, 1e+06, 2e+06, 3e+06, 4e+06, 5e+06, 6e+06]

folium.Choropleth(
    geo_data=atf_spatial_analysis_df,
    name='No. of LCWIPS and ATF Allocations 2024-2025',
    data=atf_spatial_analysis_df,
    columns=['name', 'atf24_25'],
    key_on='feature.properties.name',
    fill_color='viridis',
    fill_opacity=0.9,
    line_opacity=0.4,
    legend_name='ATF (2024-2025) Allocations',
    threshold_scale=threshold_scale,
    nan_fill_color="white"
).add_to(m)

#Add the Custom Popups
# Define the specific fields and aliases you want to display
popup_fields = ['name', 'lcwip_report_count', 'pct_mentions', 'total_funding_gbp', 'atf24_25']
aliases = [
    'Authority:',
    'LCWIP Report Count:',
    'Reports Mentioning PCT:',
    'Total LCWIP Funding:',
    'ATF 24/25 Allocation:'
]

# Add a GeoJson layer with the custom popups
popup = folium.features.GeoJsonPopup(fields=popup_fields, aliases=aliases, localize=True, labels=True)
folium.GeoJson(
    atf_spatial_analysis_df,
    name="Authority Details",
    style_function=lambda x: {'color': 'black', 'weight': 0.5, 'fillOpacity': 0.1},
    tooltip=folium.features.GeoJsonTooltip(fields=['name']),
    popup=popup
).add_to(m)

folium.LayerControl().add_to(m)

#Display the Map
m
```

```{python}
atf_spatial_analysis_df
```

```{python}
atf_spatial_analysis_df.info()
```

```{python}
# Replace 0 with NaN in the specific columns
atf_spatial_analysis_df['total_funding_gbp'] = atf_spatial_analysis_df['total_funding_gbp'].replace(0, np.nan)
atf_spatial_analysis_df['total_length_km'] = atf_spatial_analysis_df['total_length_km'].replace(0, np.nan)
```

```{python}
atf_spatial_analysis_df.info()
```

```{python}
#atf_spatial_analysis_df = atf_spatial_analysis_df.dropna()
```

## Finding relationship between ATF funding & Total funding data collected

```{python}
# Histogram for collected Total Funding
plt.subplot(1, 2, 1) 
sns.histplot(atf_spatial_analysis_df['total_funding_gbp'], bins=5, kde=True) 
plt.title('Histogram of Total Funding GBP')
plt.xlabel('Total Funding - Data Collection')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

```{python}
# Histogram for ATF Funding allocation
plt.subplot(1, 2, 1) 
sns.histplot(atf_spatial_analysis_df['atf_total_20_24'], bins=5, kde=True) 
plt.title('Histogram of ATF Allocation GBP 2020-2024')
plt.xlabel('ATF Allocation GBP 2020-2024')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

```{python}
atf_spatial_analysis_df['total_funding_gbp'].info()
```

### Getting the funding values only to check the normality and correlation

```{python}
tf_collected = atf_spatial_analysis_df['total_funding_gbp'].dropna()
tf_collected
```

```{python}
atf_total = atf_spatial_analysis_df['atf_total_20_24'].dropna()
atf_total
```

```{python}
from scipy.stats import shapiro  # Importing Shapiro-Wilk test
from scipy.stats import kruskal  # Importing Kruskal-Wallis test
from scipy.stats import ttest_ind  # Importing independent samples t-test
```

### Normality Test for the funding columns

```{python}
# Shapiro-Wilk test for total funding collected
stat_tf_collected, p_value_tf_collected = shapiro(tf_collected)
print("Shapiro-Wilk Test p-value for total_funding_gbp:", p_value_tf_collected)

# Shapiro-Wilk test for ATF funding total
stat_atf_total, p_value_atf_total = shapiro(atf_total)
print("Shapiro-Wilk Test p-value for atf_total_20_24:", p_value_atf_total)
```

Both p-values are less than 0.05, it rejects the null hypothesis and shows the data are not normally distributed. So we can proceed with a non-parametric test, Mann-Whitney U Test.

```{python}
from scipy.stats import mannwhitneyu

# Perform Mann-Whitney U test
stat_mannwhitney, p_value_mannwhitney = mannwhitneyu(tf_collected, atf_total)
print("Mann-Whitney U Test statistic:", stat_mannwhitney)
print("p-value:", p_value_mannwhitney)
```

In this case, since the p-value is relatively small (2.1.811424654663702e-09), we reject the null hypothesis and conclude that there is a significant difference between collected total funding and the total atf funding allocation from 2020 to 2024.

```{python}
correlation_matrix = atf_spatial_analysis_df[['total_funding_gbp', 'atf_total_20_24']].corr()
correlation_coefficient = correlation_matrix.iloc[0, 1]
```

```{python}
correlation_coefficient
```

```{python}
if correlation_coefficient > 0.7:
    print("This indicates a strong positive correlation.")
elif correlation_coefficient > 0.4:
    print("This indicates a moderate positive correlation.")
elif correlation_coefficient > 0:
    print("This indicates a weak positive correlation.")
elif correlation_coefficient < -0.7:
    print("This indicates a strong negative correlation.")
elif correlation_coefficient < -0.4:
    print("This indicates a moderate negative correlation.")
else:
    print("This indicates a weak or no linear correlation.")


#Visualize with a Scatter Plot
plt.figure(figsize=(10, 6))
plt.scatter(atf_spatial_analysis_df['total_funding_gbp'], atf_spatial_analysis_df['atf_total_20_24'])

#Formatting for clarity
plt.title('LCWIP Planned Cost vs. Total ATF Funding')
plt.xlabel('Total LCWIP Cost (£)')
plt.ylabel('Total ATF Funding (£)')
plt.grid(True)

#axes in millions
plt.gca().get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, p: f'£{x/1000000:.0f}M'))
plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda y, p: f'£{y/1000000:.0f}M'))

plt.show()
```

## Finding relationship between the ATF funding and the proposed length of cycle network (data collected)

```{python}
# Histogram for ATF Funding allocation
plt.subplot(1, 2, 1) 
sns.histplot(atf_spatial_analysis_df['atf_total_20_24'], bins=5, kde=True) 
plt.title('Histogram of ATF Allocation GBP 2020-2024')
plt.xlabel('ATF Allocation GBP 2020-2024')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

```{python}
# Histogram for ATF Funding allocation
plt.subplot(1, 2, 1) 
sns.histplot(atf_spatial_analysis_df['total_length_km'], bins=5, kde=True) 
plt.title('Histogram of Proposed Length of Cycle Network (km)')
plt.xlabel('Proposed Length of Cycle Network (km)')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

```{python}
atf_spatial_analysis_df['total_length_km'].info()
```

### Getting the network length only for the normality test

```{python}
network_length_km = atf_spatial_analysis_df['total_length_km'].dropna()
```

```{python}
# Shapiro-Wilk test for proposed network length
stat_network_length, p_value_network_length = shapiro(network_length_km)
print("Shapiro-Wilk Test p-value for total_funding_gbp:", p_value_network_length)
```

```{python}
# Perform Mann-Whitney U test
stat_mannwhitney_km, p_value_mannwhitney_km = mannwhitneyu(network_length_km, atf_total)
print("Mann-Whitney U Test statistic:", stat_mannwhitney_km)
print("p-value:", p_value_mannwhitney_km)
```

```{python}
correlation_matrix_km_atf = atf_spatial_analysis_df[['total_length_km', 'atf_total_20_24']].corr()
correlation_coefficient_km_atf = correlation_matrix_km_atf.iloc[0, 1]
print(correlation_coefficient_km_atf)
```

```{python}
if correlation_coefficient_km_atf > 0.7:
    print("This indicates a strong positive correlation.")
elif correlation_coefficient_km_atf > 0.4:
    print("This indicates a moderate positive correlation.")
elif correlation_coefficient_km_atf > 0:
    print("This indicates a weak positive correlation.")
elif correlation_coefficient_km_atf < -0.7:
    print("This indicates a strong negative correlation.")
elif correlation_coefficient_km_atf < -0.4:
    print("This indicates a moderate negative correlation.")
else:
    print("This indicates a weak or no linear correlation.")


#Visualize with a Scatter Plot
plt.figure(figsize=(10, 6))
plt.scatter(atf_spatial_analysis_df['total_length_km'], atf_spatial_analysis_df['atf_total_20_24'])

#Formatting for clarity
plt.title('LCWIP Propopsed Cycle Network Length vs. Total ATF Funding')
plt.xlabel('Total Propopsed Cycle Network Length (km)')
plt.ylabel('Total ATF Funding (£)')
plt.grid(True)

#y axis in millions
plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda y, p: f'£{y/1000000:.0f}M'))

plt.show()
```


